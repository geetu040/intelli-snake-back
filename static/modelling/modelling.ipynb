{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, val=True):\n",
    "\tplt.figure(figsize=(20, 4))\n",
    "\tplt.subplot(1, 2, 1)\n",
    "\tplt.plot(history['acc'])\n",
    "\tif val: plt.plot(history['val_acc'])\n",
    "\tplt.xlabel(\"epochs\")\n",
    "\tplt.ylabel(\"acc\")\n",
    "\tif val: plt.legend([\"acc\", \"val_acc\"])\n",
    "\tplt.subplot(1, 2, 2)\n",
    "\tplt.plot(history['loss'])\n",
    "\tif val: plt.plot(history['val_loss'])\n",
    "\tplt.xlabel(\"epochs\")\n",
    "\tplt.ylabel(\"loss\")\n",
    "\tif val: plt.legend([\"loss\", \"val_loss\"])\n",
    "\tplt.show()\n",
    "\tif val:\n",
    "\t\tprint(f\"\"\"\n",
    "acc      : {round(history['acc'][-1], 3)}\n",
    "val_acc  : {round(history['val_acc'][-1], 3)}\n",
    "loss     : {round(history['loss'][-1], 3)}\n",
    "val_loss : {round(history['val_loss'][-1], 3)}\n",
    "\t\"\"\")\n",
    "\telse:\n",
    "\t\tprint(f\"\"\"\n",
    "acc      : {round(history['acc'][-1], 3)}\n",
    "loss     : {round(history['loss'][-1], 3)}\n",
    "\t\"\"\")\n",
    "\n",
    " \n",
    "def process_data(dataset_url, get_maps=False, scale=True):\n",
    "\t# FETCHING THE DATASET\n",
    "\tdataset = np.load(dataset_url)\n",
    "\tx_train, x_test, y_train, y_test = dataset['x_train'], dataset['x_test'], dataset['y_train'], dataset['y_test']\n",
    "\tx = np.concatenate([x_train, x_test])\n",
    "\ty = np.concatenate([y_train, y_test])\n",
    "\t# SHUFFLING THE DATASET\n",
    "\tr = np.random.permutation(x.shape[0])\n",
    "\tx = x[r]\n",
    "\ty = y[r]\n",
    "\t# CREATING 2D ARRAY OF MAP\n",
    "\tpic_x = np.apply_along_axis(\n",
    "\t\tlambda x: x.tolist().index(1),\n",
    "\t\t3,\n",
    "\t\tx,\n",
    "\t)\n",
    "\t\t\n",
    "\t# GETTING POSSIBLE DIRECTIONS AND POSSITIONS OF HEAD AND FOOD\n",
    "\tdef get_possition_dirs(x):\n",
    "\t\tfood_x, food_y = np.where(x == 2)[1], np.where(x == 2)[0]\n",
    "\t\thead_x, head_y = np.where(x == 3)[1], np.where(x == 3)[0]\n",
    "\n",
    "\t\tpossible_dirs = [\n",
    "\t\t\tint(x[head_y-1, head_x] == 0),\n",
    "\t\t\tint(x[head_y, head_x-1] == 0),\n",
    "\t\t\tint(x[head_y+1, head_x] == 0),\n",
    "\t\t\tint(x[head_y, head_x+1] == 0),\n",
    "\t\t]\n",
    "\t\tpossitions = [head_x[0], head_y[0], food_x[0], food_y[0]]\n",
    "\t\tif scale:\n",
    "\t\t\tpossitions[0] /= 27\n",
    "\t\t\tpossitions[1] /= 30\n",
    "\t\t\tpossitions[2] /= 27\n",
    "\t\t\tpossitions[3] /= 30\n",
    "\t\treturn possitions, possible_dirs\n",
    "\tpossitions = []\n",
    "\tpossible_dirs = []\n",
    "\tfor i in pic_x:\n",
    "\t\ta = get_possition_dirs(i)\n",
    "\t\tpossitions.append(a[0])\n",
    "\t\tpossible_dirs.append(a[1])\n",
    "\tpossitions = np.array(possitions)\n",
    "\tpossible_dirs = np.array(possible_dirs)\n",
    "\n",
    "\tif get_maps:\n",
    "\t\t# GETTING MAPS\n",
    "\t\tmaps = np.where(pic_x == 1, 1, 0)\n",
    "\t\tmaps = maps.reshape(maps.shape + (1,))\n",
    "\t\treturn maps, possitions, possible_dirs, y\n",
    "\telse:\n",
    "\t\treturn possitions, possible_dirs, y\n",
    "\n",
    "def proceed_model(model, inputs, outputs, model_name, epochs=10000, verbose=0, patience=200, test_size=0.1, plot_val=True, include_callbacks=True):\n",
    "\n",
    "    if type(inputs) == list:\n",
    "        r = np.random.permutation(outputs.shape[0])\n",
    "        for i in range(len(inputs)):\n",
    "            inputs[i] = inputs[i][r]\n",
    "        outputs = outputs[r]\n",
    "\n",
    "        border_index = int(len(outputs)*(1-test_size))\n",
    "        if model_name == \"fixed\":\n",
    "            x_train = [inputs[0][:border_index], inputs[1][:border_index]]\n",
    "            x_test = [inputs[0][border_index:], inputs[1][border_index:]]\n",
    "        else:\n",
    "            x_train = [inputs[0][:border_index], inputs[1][:border_index], inputs[2][:border_index]]\n",
    "            x_test = [inputs[0][border_index:], inputs[1][border_index:], inputs[2][border_index:]]\n",
    "        y_train = outputs[:border_index]\n",
    "        y_test = outputs[border_index:]\n",
    "    else:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_size)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    if include_callbacks:\n",
    "        my_callbacks = [\n",
    "                    callbacks.EarlyStopping(patience=patience,monitor = 'acc',min_delta = 0.01,mode = 'max',),\n",
    "                    callbacks.ModelCheckpoint(filepath=models_url + model_name + '.h5',save_weights_only=True,frequency='epoch',save_freq='epoch',save_best_only=True,monitor='acc',mode='max',)\n",
    "                ]\n",
    "    else:\n",
    "        my_callbacks = None\n",
    "    with tf.device('GPU:0'):\n",
    "        history = model.fit(x_train, y_train, epochs=epochs, verbose=verbose, callbacks=my_callbacks)\n",
    "    if include_callbacks:\n",
    "        with open(models_url + model_name + \".json\", \"w\") as f:\n",
    "            f.write(model.to_json())\n",
    "    plot_history(history.history, val=plot_val)\n",
    "\n",
    "    # EVALUTAING THE MODEL\n",
    "    if include_callbacks:\n",
    "        with open(models_url + model_name + \".json\", \"r\") as f:\n",
    "            loaded_model = models.model_from_json(f.read())\n",
    "        loaded_model.load_weights(models_url + model_name + \".h5\")\n",
    "    else: loaded_model = model\n",
    "\n",
    "    loaded_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    loaded_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/content/drive/MyDrive/Cloud/Temp Files/\"\n",
    "fixed_dataset_url = \"/content/drive/MyDrive/Cloud/Temp Files//fixed_dataset.npz\"\n",
    "random_dataset_url = \"/content/drive/MyDrive/Cloud/Temp Files//random_dataset.npz\"\n",
    "binary_map_url = \"/content/drive/MyDrive/Cloud/Temp Files//binary_map.json\"\n",
    "models_url = \"/content/drive/MyDrive/Cloud/Temp Files//\"\n",
    "finalized_fixed_model_url = \"/content/drive/MyDrive/Cloud/Temp Files/Finalized/fixed_model\"\n",
    "finalized_random_model_url = \"/content/drive/MyDrive/Cloud/Temp Files/Finalized/random_model\"\n",
    "\n",
    "# GETTING THE ALREADY TRAINED MODELS\n",
    "if False:\n",
    "    with open(finalized_fixed_model_url + \".json\") as f:\n",
    "        loaded_finalized_fixed_model = models.model_from_json(f.read())\n",
    "    loaded_finalized_fixed_model.load_weights(finalized_fixed_model_url + \".h5\")\n",
    "\n",
    "    with open(finalized_random_model_url + \".json\") as f:\n",
    "        loaded_finalized_random_model = models.model_from_json(f.read())\n",
    "    loaded_finalized_random_model.load_weights(finalized_random_model_url + \".h5\")\n",
    "\n",
    "    loaded_finalized_fixed_model.compile(\"adam\", \"sparse_categorical_crossentropy\", ['acc'])\n",
    "    loaded_finalized_random_model.compile(\"adam\", \"sparse_categorical_crossentropy\", ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed_poss, fixed_dirs, fixed_y = process_data(fixed_dataset_url, get_maps=False, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSSITIONS\n",
    "input_poss = layers.Input(shape=(4,))\n",
    "model_poss = models.Sequential([\n",
    "\tlayers.Dense(20, activation='relu'),\n",
    "\t# layers.Dense(200, activation='relu'),\n",
    "\tlayers.Dense(80, activation='relu'),\n",
    "\t# layers.Dropout(0.3),\n",
    "\tlayers.Dense(400, activation='relu'),\n",
    "])(input_poss)\n",
    "output_poss = layers.Dense(10, activation='relu')(model_poss)\n",
    "\n",
    "# POSSIBLE DIRECTIONS\n",
    "input_dirs = layers.Input(shape=(4,))\n",
    "model_dirs = models.Sequential([\n",
    "\tlayers.Dense(20, activation='relu'),\n",
    "\t# layers.Dense(200, activation='relu'),\n",
    "\tlayers.Dense(80, activation='relu'),\n",
    "\t# layers.Dropout(0.3),\n",
    "\tlayers.Dense(400, activation='relu'),\n",
    "])(input_dirs)\n",
    "output_dirs = layers.Dense(10)(model_dirs)\n",
    "\n",
    "# CONCATENATED LAYERS\n",
    "input = layers.Concatenate()([\n",
    "\toutput_poss,\n",
    "\toutput_dirs,\n",
    "])\n",
    "model_concat = models.Sequential([\n",
    "\tlayers.Dense(80, activation='relu'),\n",
    "\t# layers.Dense(400, activation='relu'),\n",
    "\t# layers.Dense(1600, activation='relu'),\n",
    "\tlayers.Dense(200, activation='relu'),\n",
    "\tlayers.Dropout(0.3),\n",
    "\tlayers.Dense(800, activation='relu'),\n",
    "\tlayers.Dropout(0.3),\n",
    "\tlayers.Dense(80, activation='relu'),\n",
    "])(input)\n",
    "output = layers.Dense(4, activation='sigmoid')(model_concat)\n",
    "\n",
    "fixed_model = models.Model(inputs=[input_poss, input_dirs], outputs=output)\n",
    "\n",
    "fixed_model.summary()\n",
    "\n",
    "proceed_model(\n",
    "\t  model=fixed_model,\n",
    "\t  model_name=\"fixed_model\",\n",
    "\t  inputs=[fixed_poss, fixed_dirs],\n",
    "\t  outputs=fixed_y,\n",
    "\t  epochs=20,\n",
    "\t  verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0.985, 0.987, 0.9888\n",
    "0.998, 0.985, 0.9975\n",
    "0.995, 0.987, 0.9968\n",
    "0.997, 0.991, 0.9972\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_maps, random_poss, random_dirs, random_y = process_data(random_dataset_url, get_maps=True, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_maps = layers.Input(shape=(31, 28, 1), name=\"input_maps\")\n",
    "input_poss = layers.Input(shape=(4,), name=\"input_poss\")\n",
    "input_dirs = layers.Input(shape=(4,), name=\"input_dirs\")\n",
    "\n",
    "model_maps = models.Sequential([\n",
    "                                \n",
    "    # MAPS\n",
    "    layers.Conv2D(4, 2, padding='SAME', activation='linear'),\n",
    "    layers.Conv2D(8, 2, padding='SAME', activation='tanh'),\n",
    "    layers.AveragePooling2D(2),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(80, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(400, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1600, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(800, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(20, activation='relu'),\n",
    "\n",
    "], name=\"model_maps\")(input_maps)\n",
    "model_poss = models.Sequential([\n",
    "                                \n",
    "    # POSS\n",
    "    layers.Dense(20, activation=\"relu\"),\n",
    "    layers.Dense(400, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(20, activation=\"relu\"),\n",
    "\n",
    "], name=\"model_poss\")(input_poss)\n",
    "model_dirs = models.Sequential([\n",
    "                                \n",
    "    # DIRS\n",
    "    layers.Dense(20, activation=\"relu\"),\n",
    "    layers.Dense(400, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(20, activation=\"relu\"),\n",
    "\n",
    "], name=\"model_dirs\")(input_dirs)\n",
    "\n",
    "output_maps = layers.Dense(40, activation='relu', name=\"output_maps\")(model_maps)\n",
    "output_poss = layers.Dense(10, activation='relu', name=\"output_poss\")(model_poss)\n",
    "output_dirs = layers.Dense(10, activation='relu', name=\"output_dirs\")(model_dirs)\n",
    "\n",
    "input_conc = layers.Concatenate(name=\"input_conc\")([output_maps, output_poss, output_dirs])\n",
    "model_conc = models.Sequential([\n",
    "\n",
    "    # CONC\n",
    "    layers.Dense(80, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(400, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(800, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1600, activation='relu'),\n",
    "    layers.Dense(2400, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(4000, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(400, activation='relu'),\n",
    "    layers.Dense(80, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(20, activation='relu'),\n",
    "], name=\"model_conc\")(input_conc)\n",
    "output_conc = layers.Dense(4, activation='softmax', name=\"output_conc\")(model_conc)\n",
    "\n",
    "random_model = models.Model(inputs=[input_maps, input_poss, input_dirs], outputs=output_conc, name=\"random_model\")\n",
    "proceed_model(\n",
    "    model=random_model,\n",
    "    model_name=\"random_model\",\n",
    "    inputs=[random_maps, random_poss, random_dirs],\n",
    "    outputs=random_y,\n",
    "    epochs=10000,\n",
    "    patience=1000,\n",
    "    verbose=0,\n",
    "    test_size=0.2,\n",
    "    plot_val=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
